<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Johanna Hansen</title>
  
  <meta name="author" content="original author is Jon Barron, modified by JH">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Johanna Hansen</name>
		<br>
              </p>
	      <p>I'm a computer scientist / engineer interested in robotics, machine learning, and scientific sensors (especially when applied to earth science). Much of my research is about learning models of the physical world for use with interactive agents. 
              </p>
	      <p>
	      I am currently a Senior Staff Research Scientist at <a href="https://bostondynamics.com/atlas/"> Boston Dynamics </a> researching learned control for humanoid robots. Previously, I worked on a PhD at McGill University in the <a  href=http://cim.mcgill.ca/~mrl>Mobile Robotics Lab</a> and 
	      <a href="https://mila.quebec/">Mila</a> with Greg Dudek.  

	      During grad school, I worked part-time as a research intern at the <a href="https://research.samsung.com/aicenter_montreal">Samsung AI Center (SAIC) in Montreal</a> where I focused on dexturous manipulation with <a href="#tactile">visuotactile sensors</a>. In the summer of 2019, I interned at <a href="https://www.jpl.nasa.gov/">JPL</a> and looked at <a href="#jpl-meta">machine vision</a> <a href="#jpl-data">aspects</a> of the Mars Sample Return Mission.  Prior to starting graduate school, I was an at-sea engineer for the autonomous underwater vehicle, <a href="https://www.whoi.edu/what-we-do/explore/underwater-vehicles/auvs/sentry/">Sentry</a>, in the National Deep Submergence Facility at <a href="https://ndsf.whoi.edu/">WHOI</a> and an engineer working on <a href="#swri">applied sensing</a> at <a href="https://www.swri.org/technical-divisions/intelligent-systems">SWRI</a>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:johanna.hansen@mail.mcgill.ca">Email</a> &nbsp/&nbsp
                <a href="data/johanna_hansen_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="data/johanna-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.ca/citations?user=vDn8bXAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/johanbanan">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/johannah/">Github</a>
		<br>
		<br>
	      <a href="#research_projects">Research Projects</a> /
	      <a href="#service">Service</a> /
	      <a href="#talks">Talks</a> 
              </p>

            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/portrait.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/portrait_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td>
			<a id="ongoing">
		    <hr>
			<a id="research_projects">
              <heading>Resarch Projects</heading>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
			<a id="tactile">
              <img src="images/canjacokine_DH_abs_posquat01000000.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Learning to Manipulate from Pixels with a Kinematic Critic </papertitle>
              </a>
              <br>
            <strong>Johanna Hansen*</strong>, Kyle Kastner*, Yuying Huang, Aaron Courville, Dave Meger, and Gregory Dudek
              <br>

              <p>
	
This paper introduces a pixel-based actor-critic architecture featuring a differentiable Denavit-Hartenburg (DH) forward kinematics function in the critic sub-network, which achieves substantial improvement in average cumulative reward across several complex manipulation tasks and two robot arms in Robosuite, compared to strong baselines. Forward kinematics as described by DH parameterization for rigid-body robots is fully differentiable with respect to input joint angles, given fixed link-relative geometric information, and including this differentiable module improves training of reinforcement learning agents on an array of benchmark manipulation tasks. We show the importance of formulating a differentiable kinematic function for overall task performance in an ablation study, and demonstrate a simulation-learned policy running on a real Jaco 7DOF robot.
	       </p>
	    <p>
                <a href="papers/kinematic_critic_final.pdf">Paper</a> / 
		<a href="https://youtu.be/w4YruKjelC8">Video</a> /
		<a href="https://johannah.github.io/kinematic-critic/">Webpage</a>
	    </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drifter/example_drift.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/johannah/drift-predict">
            <papertitle> Forecasting Floats by Learning to Transfer Trajectories </papertitle>
              </a>
              <br>
              <strong>Johanna Hansen</strong>, <a href="http://www.cim.mcgill.ca/~travism/">Travis Manderson</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Greg Dudek</a>

              <br>
              <p>
	     Despite precise weather forecasting, satellite observations, and marine instrumentation of much of the ocean we know little about how currents move on the surface of the ocean. In this ongoing research effort, developed for the DARPA Forecasting Floats in Turbulence (FFT) Challenge, we predict float trajectories for 10 days in the future using a combination of physics models of the ocean, weather predictions, and transfer learning from past float trajectories (5th place in global competition). 
	       </p>
	    <p>
                <a href="https://github.com/johannah/drift-predict">Github</a> 
	    </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jaco/robo_door.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Developing a Sim2Real Interface for the Jaco Robot  </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen*</strong>, <a href="https://melfm.github.io/about.html/">Melissa Mozifian*</a>, and <a href="https://sahandrez.github.io/">Sahand Rezaei-Shoshtari</a>
              <br>
	      <p>  The goal of this project is to build a sim2real and transfer learning interface for the Jaco robot by Kinova. We have transfer frameworks which work with both <a href="https://github.com/deepmind/dm_control">DM Control Suite</a> and <a href="https://github.com/ARISE-Initiative/robosuite">Robosuite</a>. Ongoing projects which utilize these tools include: <a href="https://sites.google.com/view/ibit">Intervention Design for Effective Sim2Real Transfer (IBIT)</a> and <a href="https://github.com/johannah/DH">Transfer Learning with Differentiable Physics-Informed Priors</a>.
	       </p>
	    <p>
		<a href="https://github.com/johannah/jaco_docker">Docker</a>  /
		<a href="https://github.com/johannah/ros_interface">ROS Interface</a> /
		<a href="https://github.com/johannah/jaco_rl">RL Example</a> /
		<a href="https://github.com/melfm/ibit">IBIT</a> /
		<a href="https://github.com/melfm/robosuite">Robosuite</a> 
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/model-based-rl/agent_grad.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/johannah/planning_on_models">
            <papertitle> Model-Based Planning with Discrete Latent States</papertitle>
              </a>
              <br>
              <strong>Johanna Hansen</strong> and <a href="http://kastnerkyle.github.io/">Kyle Kastner</a>
              <br>
              <p>
	       In this project, we investigate planning on imagined futures which consist of learned discrete states. We achieve scores of +30 on the hard exploration task of Freeway with a pre-trained world model. In the thumbnail, we show our agent with GradCam on the agent. Notice how nearby cars influence actions and the gradient. Ongoing work which builds on the following research threads of exploration and associative generative modeling.  
	       </p>
	    <p>
                <a href="https://github.com/johannah/planning_on_models">Github</a> 
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bootstrap_breakout_winning_gaps.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/johannah/bootstrap_dqn">
            <papertitle> Exploration for Model-Based Reinforcement Learning </papertitle>
              </a>
              <br>
              <strong>Johanna Hansen</strong> and <a href="http://kastnerkyle.github.io/">Kyle Kastner</a>
              <br>
              <p>In the interest of learning more about exploration in reinforcement learning, we set out to implement two papers on exploration by Ian Osband et al.:  <a href="https://arxiv.org/abs/1602.04621">Deep Exploration via Bootstrap DQN</a> and <a href="https://arxiv.org/abs/1806.03335">Randomized Prior Functions for Deep Reinforcement Learning</a>. We evaluate our implementation in the Atari framework and investigate a model-based and double-Q learning version of the work, matching SOTA results. Ongoing work.  
	       </p>
	    <p>
                <a href="https://github.com/johannah/bootstrap_dqn">Github</a> /
                <a href="https://docs.google.com/presentation/d/1hSh-7y3ihNdy5rY88bcwu_CeBNCHzZ3HeywJ_54umjw/edit?usp=sharing">Slides</a>
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acn_neighbor.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Learning to Associate Discrete Latents  </papertitle>
              </a>
	      <br>
	      <strong>Johanna Hansen*</strong> and <a href="http://kastnerkyle.github.io/">Kyle Kastner*</a>
              <p>  In this generative modeling project, we implement Associative Compression Networks for Representation Learning (ACN) by Graves, Menick, and van den Oord. We also introduced a VQ-VAE style decoder to the ACN model for discrete latents and call this architecture ACN-VQ. In the thumbnail, we show how to encode an example from the validation set (leftmost image) and find its nearest neighbors (right columns) according to the learned ACN model. Ongoing work to apply this in exploration for reinforcement learning settings. 
	       </p>
	    <p>
		<a href="https://github.com/johannah/ACN">Github</a>  /
		<a href="https://johannah.github.io/images/acn/fashion_acn_validation_00_0032400000ex_pca_valid.html">ACN Demo</a>  / 
		<a href="https://johannah.github.io/images/acn/fashion_acnvq_validation_small_vq_01_0078000000ex_pca_valid.html"> ACN-VQ Demo</a> 
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
			<a id="tactile">
              <img src="images/visuotactile2.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Visuotactile-RL: Learning Multimodal Manipulation Policies with Deep Reinforcement Learning </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://fhogan.github.io"/>Francois Hogan</a>, <a href="https://scholar.google.com/citations?user=YZMnju8AAAAJ&hl=en">Dmitriy Rivkin</a>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="https://lassonde.yorku.ca/users/jenkin">Michael Jenkin</a>, and <a href="https://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
       <p> 
	    <em>Presented at ICRA 2022, Philadelphia, PA</em>
       </p>
  
              <p>
	      Manipulating objects with dexterity requires timely
feedback that simultaneously leverages the senses of vision
and touch.  In this paper, we focus on the problem setting
where both visual and tactile sensors provide pixel-level
feedback for Visuotactile reinforcement learning agents.
We investigate the challenges associated with multimodal
learning and propose several improvements to existing RL
methods; including tactile gating, tactile data
augmentation, and visual degradation. When compared with
visual-only and tactile-only baselines, our Visuotactile-RL
agents showcase (1) significant improvements in
contact-rich tasks; (2) improved robustness to visual
changes (lighting/camera view) in the workspace; and (3)
resilience to physical changes in the task environment
(weight/friction of objects). 
	       </p>
	       <p>
                <a href="papers/Visuotactile-RL.pdf">Paper</a> 
	       </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/boat/sample-at-depth-thumbnail.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle>An Autonomous Probing System for Collecting Measurements at Depth</papertitle>
              </a>
              <br>
	      <a href="https://www.linkedin.com/in/yuyingblairhuang/">Yuying (Blair) Huang</a>,
       <a href="https://www.linkedin.com/in/yiming-yao-a082b7107/">Yiming Yao</a>,
       <strong> Johanna Hansen </strong>,
       <a href="https://www.linkedin.com/in/jeremy-mallette/?originalSubdomain=ca">Jeremy Mallette</a>,
       <a href="https://www.cim.mcgill.ca/~msandeep/">Sandeep Manjanna</a>,
       <a href="https://www.cim.mcgill.ca/~dudek/">Greg Dudek</a>, and
       <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>
       <p> 
	    <em> Selected for Poster Competition (Top ~10% Student Paper) at OCEANS 2021, San Diego </em>
       </p>
              <p>
	      This paper presents the portable autonomous probing system (APS), a low-cost robotic design for collecting water quality measurements at targeted depths from an autonomous surface vehicle (ASV). This system fills an important, but often overlooked niche in marine sampling by enabling mobile sensor observations throughout the near-surface water column without the need for advanced underwater equipment. 

	       </p>

	    <p>
                <a href="papers/sample-at-depth.pdf">Paper</a> / 
		<a href="https://github.com/edwardming789/winch_controller">Github</a> /
		<a href="https://www.youtube.com/embed/b3xo9Kt7zHk">Video</a> /
		<a href="https://johannah.github.io/publication/sample-at-depth/index.html">Webpage</a>
	    </p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jpl/ral2021.jpg" alt="clean-usnob" width="160" height="180">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Rover Relocalization for Mars Sample Return by Virtual Template Synthesis and Matching </papertitle>
              </a>
              <br>
	     <a href="https://hoa.pm/">Tu-Hoa Pham</a>, William Seto, Shreyansh Daftry, Barry Ridge, <strong> Johanna Hansen </strong>, Tristan Thrush, Mark Van der Merwe, Gerard Maggiolino, Alexander Brinkman, John Mayo, Yang Cheng, Curtis Padgett, Eric Kulczycki, <a href="http://renaud-detry.net/">Renaud Detry</a>
              <br>
	      <p>
	    <em>Accepted to IEEE Robotics and Automation Letters, 2021 </em>
              <p>
	      </p>
              <p> 
	     We consider the problem of rover relocalization in
the context of the notional Mars Sample Return campaign. In
this campaign, a rover (R1) needs to be capable of autonomously
navigating and localizing itself within an area of approximately
50 × 50 m using reference images collected years earlier by
another rover (R0). We propose a visual localizer that exhibits
robustness to the relatively barren terrain that we expect to
find in relevant areas, and to large lighting and viewpoint
differences between R0 and R1. 
	       </p>
	    <p>
                <a href="https://arxiv.org/pdf/2103.03395.pdf">Paper</a> 
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drifter/drifter_catch.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle>Autonomous marine sampling enhanced by strategically deployed drifters in marine flow</papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="http://cim.mcgill.ca/~/msandeep">Sandeep Manjanna</a>, <a href="https://sites.google.com/view/albertoq">Alberto Quattrini Li</a>,  <a href="https://www.cse.sc.edu/~yiannisr/">Ioannis Rekleitis</a>, <a href="https://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
	      <p>

	    <em> Selected for Poster Competition (Top ~10% Student Paper) at OCEANS 2018, Charleston </em>
	      </p>
              <p> 
	      We present a transportable system for ocean observations in which a small autonomous surface vehicle (ASV)
adaptively collects spatially diverse samples with aid from a
team of inexpensive, passive floating sensors known as drifters.
Drifters can provide an increase in spatial coverage at little cost
as they are propelled about the survey area by the ambient
flow field instead of with actuators. Our iterative planning
approach demonstrates how we can use the ASV to strategically
deploy drifters into points of the flow field for high expected
information gain, while also adaptively sampling the space. 
	       </p>
	    <p>
		<a href="https://arxiv.org/pdf/1811.10103.pdf">Paper</a>  
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/model-based-rl/vq-model.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Planning in Dynamic Environments with Conditional Autoregressive Models  </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen*</strong>, <a href="http://kastnerkyle.github.io/">Kyle Kastner*</a>, <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>, and <a href="https://mila.quebec/en/person/gregory-dudek/">Greg Dudek</a>
	      <p>
	      <em> Presented in the Prediction and Generative Modeling in Reinforcement Learning workshop at ICML 2018.</em>
	      </p>
              <p> We introduce MCTS planning over learned discrete latent states with conditional autoregressive forward planning.  This work demonstrates an agent solving a complicated planning problem with search using purely learned models. Long-term search on rollouts is enabled via a highly-accurate PixelCNN forward model and exact matching between states represented with VQ-VAE discrete latents.
	       </p>
	    <p>
                <a href="papers/icml18-vqvae-model-camera-ready.pdf">Paper</a> / 
		<a href="https://github.com/johannah/trajectories">Github</a> /
		<a href="https://imgur.com/a/6DJbrB1">Examples</a>
	    </p>
            </td>
          </tr>





          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drifter/iros.gif" alt="clean-usnob" width="160" height="200">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Coverage optimization with non-actuated, floating mobile sensors using iterative trajectory planning in marine flow fields </papertitle>
              </a>
              <br>
	      <strong> Johanna Hansen </strong> and <a href="http://cim.mcgill.ca/~dudek">Greg Dudek</a>
              <br>
	      <p>
<em>Presented at IROS 2018</em>
	      </p>
              <p> 
	      This paper considers a spatial coverage problem in which a network of passive floating sensors is used to collect samples in a body of water. We employ an iterative measurement and modeling scheme to incrementally deploy sensors so as to achieve spatial coverage, despite only controlling the initial sample point. Once deployed, sensors are moved about a survey area by ambient surface currents.
	       </p>
	    <p>
                <a href="https://www.cim.mcgill.ca/~mrl/pubs/jhansen/IROS2018.pdf">Paper</a> 
	    </p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/walker_foot.png" alt="clean-usnob" width="160" height="140">
            </td>
            <td width="75%" valign="middle">
            <papertitle>Benchmark Environments for Multitask Learning in Continuous Domains</papertitle>
              </a>
              <br>
	      <a href="https://www.peterhenderson.co/#about">Peter Henderson</a>, <a href="https://weidi-chang.github.io/">Wei-Di Chang</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>, <strong>Johanna Hansen</strong>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="https://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
	      <p>
	      <em> Presented at the Lifelong Learning Workshop at ICML 2017</em>
	      </p>
              <p> 
	      This work presents a multitask simulation environment based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.
	       </p>
	    <p>
		<a href="https://arxiv.org/abs/1708.04352">Paper</a>  /
		<a href="https://drive.google.com/file/d/1RabKVbauf2k_rKs-z_DLoHMy6I3H1HMR/view?usp=sharing">Slides</a>  / 
		<a href="https://github.com/Breakend/gym-extensions">Github</a> 
	    </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drifter/crv.png" alt="clean-usnob" width="160" height="140">
            </td>
            <td width="75%" valign="middle">
            <papertitle>Collaborative Sampling Using Heterogeneous Marine Robots Driven by Visual Cues</papertitle>
              </a>
              <br>
	      <a href="http://cim.mcgill.ca/~/msandeep">Sandeep Manjanna</a>, <strong>Johanna Hansen</strong>, <a href="https://sites.google.com/view/albertoq">Alberto Quattrini Li</a>,  <a href="https://www.cse.sc.edu/~yiannisr/">Ioannis Rekleitis</a>, <a href="https://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
	      <p>
	      <em> Presented at the Computer Robot Vision (CRV) Conference in 2017</em>
	      </p>
              <p> 
	      We present
a method to strategically sample locally observable features
using two classes of sensor platforms. Our system consists
of a sophisticated autonomous surface vehicle (ASV) which
strategically samples based on information provided by a
team of inexpensive sensor nodes. The sensor nodes effectively
extend the observational capabilities of the vehicle by capturing
georeferenced samples from disparate and moving points across
the region. The ASV uses this information, along with its own
observations, to plan a path so as to sample points which
it expects to be particularly informative.
	       </p>
	    <p>
		<a href="https://www.cim.mcgill.ca/~mrl/pubs/sandeep/CRV2017.pdf">Paper</a>  
	    </p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drifter/combine.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle>Data Correlation and Comparison from Multiple Sensors Over a Coral Reef with a Team of Heterogeneous Aquatic Robots</papertitle>
              </a>
              <br>
	      <a href="https://sites.google.com/view/albertoq">Alberto Quattrini Li</a>,  <a href="https://www.cse.sc.edu/~yiannisr/">Ioannis Rekleitis</a>, <a href="http://cim.mcgill.ca/~/msandeep">Sandeep Manjanna</a>, <a href="https://ca.linkedin.com/in/nikhil-kakodkar-48187130">Nikhil Kakodkar</a>, <strong>Johanna Hansen</strong>, <a href="https://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>, Leonardo Bobadilla, Jacob Anderson, <a href="http://www.ryannealsmith.com/">Ryan N. Smith</a>
              <br>
	      <p>
	      <em> Presented at the International Symposium on Experimental Robotics (ISER 2016)</em>
	      </p>
              <p> 
	      This paper presents experimental insights from the deployment of an ensemble of heterogeneous autonomous sensor systems over a shallow coral reef. Visual, inertial, GPS, and ultrasonic data collected are compared and correlated to produce a comprehensive view of the health of the coral reef. Coverage strategies are discussed with a focus on the use of informed decisions to maximize the information collected during a fixed period of time.
	       </p>
	    <p>
		<a href="https://link.springer.com/chapter/10.1007/978-3-319-50115-4_62">Paper</a>  
	    </p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sentry_mule.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle>Autonomous Acoustic-Aided Optical Localization for Data Transfer</papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://scholar.google.com/citations?hl=en&user=pTcB5WUAAAAJ">Dehann Fourie</a>, <a href="https://scholar.google.com/citations?user=AqdpGVwAAAAJ&hl=en">James Kinsey</a>, Clifford Pontbriand,
	      John Ware, <a href="https://www.whoi.edu/profile/nfarr/">Norm Farr</a>, <a href="https://scholar.google.com/citations?user=w8KhSjsAAAAJ&hl=en">Carl Kaiser</a>, <a href="https://www.whoi.edu/profile/mtivey/">Maurice Tivey</a>

              <br>
	      <p>
	      <em> Presented at OCEANS 2015</em>
	      </p>
              <p> 
	      The emergence of high speed optical communication systems has introduced a method for transferring data relatively quickly underwater. This technology coupled with autonomous underwater vehicles (AUVs) has the potential to enable efficient wireless data transfers underwater. Data muling, a data transport mechanism in which AUVs visit remote sensor nodes to transfer data, enables remote data to be recovered cheaply from underwater sensors. This paper details efforts to develop a system to reduce operational complexities of autonomous data-muling. We report a set of algorithms, systems, and experimental results of a technique to localize a sub-sea sensor node equipped with acoustic and optical communication devices with an AUV. 
	       </p>
	    <p>
	    <a href="https://ieeexplore.ieee.org/abstract/document/7401982">Paper</a> /
		<a href="https://docs.google.com/presentation/d/1rbUESNq9c7YgFMLSOjuiV1Pd3Cm_Xpyw4YRnwqEjk_4/edit?usp=sharing">Slides</a>  
	    </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	<a id="swri">
              <img src="images/swri_probe.gif" alt="clean-usnob" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
            <papertitle>The Ultrasonic Culvert Inspection System (UCIS): A Low-Cost Device for Conduit Inspection</papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://www.linkedin.com/in/gregwillden">Greg Willden</a>, <a href="https://www.stmarytx.edu/academics/faculty/ben-abbott/">Ben Abbott</a>, and <a href="https://www.linkedin.com/in/ronald-green-04a15321">Ronald Green</a>

              <br>
	      <p>
	      <em> Presented at the Transportation Research Board (TRB) Meeting, 2014</em>
	      </p>
              <p> 
	      Southwest Research Institute® (SwRI) and the Federal Highway Administration (FHWA) teamed to create a low-cost sensor to inspect in-service culverts in both wet and dry conditions. This sensor system, called the Ultrasonic Culvert Inspection System (UCIS) is suitable for mapping, monitoring, and diagnosing damage to roadway culverts using sonar mapping and live video. The sensor allows the inspector to evaluate culverts with minimal time, equipment, and cost to the sponsoring agency. Sonar information, collected as the probe travels through the culvert, can be combined with inertial measurement and distance data to produce a three-dimensional representation of the culvert that can be manipulated and viewed from many angles. A specially calibrated sonar scheme allows the <a href="http://github.com/johannah/johannah.github.io/tree/master/images/annotatedprobe.png">sensor</a> to be developed with inexpensive components, making this system appropriate for use in high-risk, flooded inspections. 
	       </p>
	    <p>
	    <a href="https://trid.trb.org/view.aspx?id=1287406">Paper</a> /
		<a href="https://www.youtube.com/watch?v=ZDixIOjpiks">Video</a>  
	    </p>
            </td>
          </tr>






			<a id="jpl-data">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jpl/jpl_mars_yard.jpeg" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Development of Mars Sample Return Visual Dataset  </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong> and JPL Vision Team 
              <br>
	      <p> 
	     We worked with Mars geology specialists to develop a realistic visual dataset of sample return tubes. This dataset development campaign, which we undertook over multiple weeks, utilized a Vicon tracking system for tube localization under various natural illumination settings and rock formations. 
	       </p>
            </td>
          </tr>

          <tr>
			<a id="jpl-meta">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jpl/export.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Meta-Modeling for Efficient Expert Choice
in Sample Tube Re-Localization  </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://hoa.pm/">Tu-Hoa Pham</a>, and <a href="http://renaud-detry.net/">Renaud Detry</a>
              <br>
	      <p> 
	      NASA is studying concepts for returning physical samples from Mars. One notional aspect of such a campaign would involve a rover that would retrieve tubes containing samples of the Martian terrain that were stored on the surface of Mars in a previous mission. In this work, we consider the problem of developing a vision-based sample tube re-localization capability to facilitate autonomous sample tube grasping and retrieval. Broadly, there are two algorithmic approaches or experts we consider for the task of re-localization. The terrain-relative expert finds a geometric transformation between images collected during sample tube deployment and the rover’s current observations, allowing it to find the tube based on human annotations of the deployment image. The direct expert localizes the tube using only current observations. These two experts generally have orthogonal performance cases, with terrain-relative methods failing under large changes in viewpoint, scale, or illumination and direct methods unable to succeed under tube occlusion. Our work seeks to learn to exploit these performance differences by building several experts of each class and a meta-model which learns to identify the most performant expert for a given sample-tube collection assignment. Ideally, our meta-model will be lightweight, robust, and reduce computational cost by allowing us to contextually execute experts. In this report, we demonstrate performance in a variety of state-of-the-art experts and present results with ground-truth localization. In addition, we provide initial meta-modeling results that give insight into the direction of future work on the topic.
	       </p>
	       <p>

             <a href="images/jpl/released/4Oct2019_Meta_Modeling_for_Efficient_Expert_Choicein_Sample_Tube_Re_Localization.pdf">Project Report</a>
	       </p>
            </td>
          </tr>











          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lake_life/000091.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Learning to Classify Zooplankton  </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://scholar.google.ca/citations?user=NU6iGfEAAAAJ&hl=fr">Riwan Leroux</a>, and <a href="https://oraprdnt.uqtr.uquebec.ca/pls/public/genw050r.page_perso?owa_no_personne=244253">Andrea Bertolo</a>
              <br>
	      <p>  The goal of this project was to build a tool to easily classify and detect zooplankton in Canadian Lakes. We trained a hierarchical ResNet <a href="http://johannah.github.io/images/lake_life/ckptwt_eval00160_train_normalized_confusion.png">classifier</a> capable of handling this unbalanced dataset and tuned an <a href="https://github.com/johannah/ACN/">ACN</a> to enable robust pixel-based nearest-neighbor lookups to assist in labeling new species. 
	       </p>
	    <p>
		<a href="https://github.com/johannah/lake_life">Github</a> 
	    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iceberg/108blue.gif" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Volume Estimates of Icebergs from Imperfect Observations  </papertitle>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://web.uri.edu/gso/meet/mingxi-zhou-ph-d/">Mingxi Zhou</a>, and <a href="https://www.marum.de/en/Prof.-Dr.-ralf-bachmayer.html">Ralf Bachmayer</a>
              <br>
	      <p> This work sought to understand how icebergs change shape over time. Over a series of days, we collected both acoustic observations from under the water via an <a href="http://github.com/johannah/johannah.github.io/tree/master/images/iceberg/deploy_glider.jpeg">underwater glider</a> and <a href="https://github.com/johannah/johannah.github.io/blob/master/images/iceberg/collect_img.jpg">unstructured images</a> from the part of the iceberg visible above the water. I trained a <a href="https://github.com/phillipi/pix2pix">pix2pix model</a> to segment the iceberg images from limited labels and then used standard SFM tools to create a <a href="http://github.com/johannah/johannah.github.io/tree/master/images/iceberg/iceberg_sfm.png">3D volume estimate</a> of the air-exposed portion of the iceberg. Later the air and water estimates were stitched together to <a href="http://ncfrn.mcgill.ca/members/pubs/Iceberg_Mingxi.pdf">estimate iceberg volume/>. 
	       </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bixi.gif" alt="clean-usnob" width="160" height="200">
            </td>
            <td width="75%" valign="middle">
            <papertitle> Understanding Community Bikeshare Dynamics </papertitle>
              </a>
              <br>
	      <strong>Johanna Hansen</strong>, <a href="https://www.elisagmferreira.com/">Elisa Ferreira</a>, <a href="https://ca.linkedin.com/in/juliette-lavoie/en">Juliette Lavoie</a>, <a href="https://ca.linkedin.com/in/julietseng">Julie Tseng</a>, <a href="https://jasminew.me/">Jasmine Wang</a>
              <br>
	      <p>  
	      In this AI4Good hackathon project, we sought to understand how Montreal's bikesharing program influenced transportation, particularly in low-income comminities. We show how reliable bike/station availability can increase the likelihood of a neighborhood's residents to commute by bike. 
	       </p>
	    <p>
		<a href="https://github.com/johannah/mtl-bixi">Github</a> /
		<a href="https://docs.google.com/presentation/d/1o5vMtLAqRZyFDDVCqUDl0y_NPa86FQCj7Fn4aLZiEO8/edit?usp=sharing">Slides</a> 
	    </p>
            </td>
          </tr>



          <tr>
            <td>

		    <hr>
<heading>Service</heading>
<a id="service">
            </td>
          </tr>
 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ai4earthlogo.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
    <papertitle> <a href="https://ai4earthscience.github.io">AI4Earth Organizer</a> </papertitle>
              <br>
	      <p>
	      <em>ICLR 2020 and NeurIPS 2020</em> 
	      </p>
              <p>Organizer for the AI4Earth machine learning workshop. This workshop highlights work being done at the intersection of AI and the Earth and Space Science. In 2020, I organized the "Sensors and Sampling" track and managed the website. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/phiflow.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://montrealrobotics.ca/diffcvgp/">Differentiable vision, graphics, and physics in machine learning (DiffCVGP) Organizer</a>
              <br>
              <em>NeurIPS 2020</em>
	      <p>Organizer for the DiffCVGP workshop and facilitator of the Mentorship Program and Gather.town Poster Session. This workshop brings together research in vision, graphics, and physics where we can explicitly encode our knowledge of the rules of the world in the form of differentiable programs. (Thumbnail courtesy of <a href="https://github.com/tum-pbs/PhiFlow">PhiFlow</a>, which was one of my favourite papers presented at the workshop.) </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros_workshop/iros_workshop_glacier.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://scientific-sampling-robots.github.io/iros-2019-workshop/">Informed Scientific Sampling in Large-scale Outdoor Environments (ISSLOE) Organizer</a>
              <br>
              <em>IROS 2019</em>
	      <p>Organizer for the ISSLOE workshop at IROS 2019  where we discussed recent progress in the field of robotic sampling for monitoring large-scale outdoor environments such as forests, bodies of water, agricultural fields, and complex urban settings. Thumbnail is from the amazing invited talk from <a href="https://asl.ethz.ch/the-lab/people/person-detail.MjA4NzU4.TGlzdC8xNTg0LDEyMDExMzk5Mjg=.html">Thomas Stastny</a> titled "Monitoring Glaciers Beyond the Horizon". 
	      </p>
            </td>
          </tr>


          <tr>
            <td>

		    <hr>
<heading>Talks</heading>
<a id="talks">
            </td>
          </tr>
 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/model-based-rl.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <br>
              <papertitle>Invited Lecture on Model-Based Reinforcement Learning at AI4Good Summer School</papertitle>
	      <p>
             <a href="https://drive.google.com/file/d/14yYn6JUTg2NZTHrbcwl1rtNwFLUJx7rq/view?usp=sharing">Slides</a>
	      </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gril.jpeg" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <br>
              <papertitle>2019 Invited Talk on Marine Robotics at Le Groupe de Recherche Interuniversitaire en Limnologie (GRIL) Symposium</papertitle> 
	      <p>
             <a href="https://docs.google.com/presentation/d/1lgfJQe1QZPRVWxXuoajPJ4gCrXffSxRlEQbmezkBCAM/edit?usp=sharing">Slides</a> 
	      </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pyladies_logo.png" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <br>
              <papertitle>Invited talk by PyLadies-MTL on Velo-Vamos!, our approach to forecasting bike shares.</papertitle>
	      <p>
             <a href="https://docs.google.com/presentation/d/1o5vMtLAqRZyFDDVCqUDl0y_NPa86FQCj7Fn4aLZiEO8/edit?usp=sharing">Slides</a>
	      </p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/deep-desc.png" alt="clean-usnob" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <br>
              <em>McGill Reading Group 2016: Deep-Desc</em>
	      <p>
             <a href="https://docs.google.com/presentation/d/10tXAZ8qZdBZyx-j7skmtLxaR0eroXQWAHjKkafUsEM8/edit?usp=sharing">Slides</a>
	      </p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/seafloor.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <br>
              <em>SciPy 2015 Talk: Seafloor Characterization with Python as a Toolbox</em>
	      <p>
             <a href="https://docs.google.com/presentation/d/1-zACrR74KUvXAvRLp3z3wu6ah7seZhjm_Qlpo64r1GQ/edit?usp=sharing">Slides</a> /
             <a href="https://www.youtube.com/watch?v=vE2TnnKdNso">Video</a>
	      </p>

            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
	      This website template was stolen from <a href="https://jonbarron.info">Jon Barron</a> using this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Others who use this code are linked here: 

                <a href="https://tnq177.github.io/">&#10025;</a>
                <a href="https://jonathan-schwarz.github.io/">&#10025;</a>
                <a href="http://www.ronnieclark.co.uk/">&#10025;</a>
                <a href="http://www.aparnadhinakaran.com/">&#10025;</a>
                <a href="https://zjysteven.github.io/">&#10025;</a>
                <a href="http://aakash30jan.github.io/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~shelhamer/">&#10025;</a>
                <a href="https://junaidcs032.github.io/">&#10025;</a>
                <a href="https://liangxuy.github.io/">&#10025;</a>
                <a href="http://www.public.asu.edu/~ssarka18/">&#10025;</a>
                <a href="https://www.cs.toronto.edu/~jennachoi/">&#10025;</a>
                <a href="https://amankhullar.github.io/">&#10025;</a>
                <a href="https://vijayvee.github.io/">&#10025;</a>
                <a href="https://anmolgoel.dev/">&#10025;</a>
                <a href="https://chanh.ee/">&#10025;</a>
                <a href="https://jefflai108.github.io/">&#10025;</a>
                <a href="https://pvskand.github.io/">&#10025;</a>
                <a href="https://ariostgx.github.io/website/">&#10025;</a>
                <a href="https://www.cs.utexas.edu/~shreyd/">&#10025;</a>
                <a href="https://www.cedrick.ai/">&#10025;</a>
                <a href="https://onlytailei.github.io/">&#10025;</a>
                <a href="https://people.cs.vt.edu/liminyang/">&#10025;</a>
                <a href="https://alexander-kirillov.github.io/">&#10025;</a>
                <a href="https://sites.cs.ucsb.edu/~yanju/">&#10025;</a>
                <a href="https://www.cct.lsu.edu/~cliu/">&#10025;</a>
                <a href="https://sid2697.github.io/">&#10025;</a>
                <a href="https://brendonjeromebutler.com/">&#10025;</a>
                <a href="https://bthananjeyan.github.io/">&#10025;</a>
                <a href="https://leonidk.com/">&#10025;</a>
                <a href="http://bopeng.space/">&#10025;</a>
                <a href="https://gautamigolani.github.io/">&#10025;</a>
                <a href="https://rohitrango.github.io/">&#10025;</a>
                <a href="https://prithv1.github.io/">&#10025;</a>
                <a href="https://www.idiap.ch/~tpereira/">&#10025;</a>
                <a href="https://felipefelixarias.github.io/">&#10025;</a>
                <a href="https://sophieschau.github.io/">&#10025;</a>
                <a href="https://wensun.github.io/">&#10025;</a>
                <a href="http://cs.umanitoba.ca/~kumarkm/">&#10025;</a>
                <a href="https://sourav-roni.github.io/">&#10025;</a>
                <a href="https://parskatt.github.io/">&#10025;</a>
                <a href="https://rauldiaz.github.io/">&#10025;</a>
                <a href="https://uzeful.github.io/">&#10025;</a>
                <a href="https://roeiherz.github.io/">&#10025;</a>
                <a href="http://relh.net/">&#10025;</a>
                <a href="https://ars-ashuha.ru/">&#10025;</a>
                <a href="https://prieuredesion.github.io/">&#10025;</a>
                <a href="https://tsujuifu.github.io/">&#10025;</a>
                <a href="https://pranoy-k.github.io/website/">&#10025;</a>
                <a href="https://xiaoleiz.github.io/">&#10025;</a>
                <a href="https://users.ece.cmu.edu/~bahn/">&#10025;</a>
                <a href="http://eracah.github.io/">&#10025;</a>
                <a href="https://adityakusupati.github.io/">&#10025;</a>
                <a href="https://agadetsky.github.io/">&#10025;</a>
                <a href="https://coh1211.github.io/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~kevinlin/">&#10025;</a>
                <a href="https://bucherb.github.io/">&#10025;</a>
                <a href="https://naman-ntc.github.io/">&#10025;</a>
                <a href="https://nicolay-r.github.io/">&#10025;</a>
                <a href="https://sakshambassi.github.io/">&#10025;</a>
                <a href="https://maheshmohanmr.github.io/publications/">&#10025;</a>
                <a href="http://byang.org/">&#10025;</a>
                <a href="https://sylqiu.github.io/">&#10025;</a>
                <a href="https://www.cc.gatech.edu/~sfoley30/">&#10025;</a>
                <a href="https://crockwell.github.io/">&#10025;</a>
                <a href="https://kritikalcoder.github.io/">&#10025;</a>
                <a href="https://isrugeek.github.io/">&#10025;</a>
                <a href="https://wei-ying.net/">&#10025;</a>
                <a href="http://people.csail.mit.edu/liuyingcheng/">&#10025;</a>
                <a href="https://itsreddy.github.io/">&#10025;</a>
                <a href="https://aditimavalankar.github.io">&#10025;</a>
                <a href="https://www.cs.ubc.ca/~zhenanf/">&#10025;</a>
                <a href="https://trinkle23897.github.io/cv/">&#10025;</a>
                <a href="https://tgangwani.github.io/">&#10025;</a>
                <a href="https://hannahlawrence.github.io/">&#10025;</a>
                <a href="http://bingxu.tech/">&#10025;</a>
                <a href="http://apjacob.me/">&#10025;</a>
                <a href="https://www.ee.iitb.ac.in/student/~krishnasubramani/">&#10025;</a>
                <a href="https://gowthamkuntumalla.github.io/">&#10025;</a>
                <a href="https://dkkim93.github.io/">&#10025;</a>
                <a href="https://chaitanyamalaviya.github.io/">&#10025;</a>
                <a href="https://daochenw.github.io/">&#10025;</a>
                <a href="https://acvictor.github.io/">&#10025;</a>
                <a href="https://tarund1996.github.io/">&#10025;</a>
                <a href="http://www-personal.umich.edu/~belz/">&#10025;</a>
                <a href="http://lauredelisle.com/">&#10025;</a>
                <a href="https://tsly123.github.io/">&#10025;</a>
                <a href="https://vanditg.github.io/">&#10025;</a>
                <a href="https://nhoma.github.io/">&#10025;</a>
                <a href="http://www.pedro.opinheiro.com/">&#10025;</a>
                <a href="https://suvan.sh/">&#10025;</a>
                <a href="https://www.diptanu.com/">&#10025;</a>
                <a href="https://yimengli46.github.io/">&#10025;</a>
                <a href="https://gauravparmar.com/">&#10025;</a>
                <a href="https://cchoquette.github.io/">&#10025;</a>
                <a href="https://psyche-mia.github.io/">&#10025;</a>
                <a href="https://mvp18.github.io/">&#10025;</a>
                <a href="https://markfzp.github.io/">&#10025;</a>
                <a href="https://theultramarine19.github.io/">&#10025;</a>
                <a href="https://sairajk.github.io/">&#10025;</a>
                <a href="https://paritoshparmar.github.io/">&#10025;</a>
                <a href="https://purvak-l.github.io/">&#10025;</a>
                <a href="https://davideabati.info/">&#10025;</a>
                <a href="https://ethanperez.net/">&#10025;</a>
                <a href="http://mdai.me/">&#10025;</a>
                <a href="https://nitish-nagesh.github.io/">&#10025;</a>
                <a href="https://duroz.github.io/">&#10025;</a>
                <a href="http://people.uncw.edu/dogang/">&#10025;</a>
                <a href="http://hal9000.space">&#10025;</a>
                <a href="http://web.stanford.edu/~yjiang05/">&#10025;</a>
                <a href="https://pratulsrinivasan.github.io/">&#10025;</a>
                <a href="http://linuxdeveloper.io">&#10025;</a>
                <a href="http://umiacs.umd.edu/~kdbrant">&#10025;</a>
                <a href="https://adityassrana.github.io/blog/about">&#10025;</a>
                <a href="https://mpalrocks.github.io/">&#10025;</a>
                <a href="https://victor7246.github.io/">&#10025;</a>
                <a href="https://www.haozhu-wang.com/">&#10025;</a>
                <a href="https://rajat499.github.io/">&#10025;</a>
                <a href="https://senya-ashukha.github.io/">&#10025;</a>
                <a href="https://qiminchen.github.io/">&#10025;</a>
                <a href="https://mark12ding.github.io/">&#10025;</a>
                <a href="https://zsb87.github.io/">&#10025;</a>
                <a href="https://yashbhalgat.github.io/">&#10025;</a>
                <a href="https://r0cketr1kky.github.io/">&#10025;</a>
                <a href="https://jaydeepborkar.github.io/">&#10025;</a>
                <a href="https://lidq92.github.io/">&#10025;</a>
                <a href="https://statho.github.io/">&#10025;</a>
                <a href="https://ankit-kaul.github.io/">&#10025;</a>
                <a href="https://jlidard.github.io/">&#10025;</a>
                <a href="http://harshpanwar.me/">&#10025;</a>
                <a href="https://c0ldstudy.github.io/about/">&#10025;</a>
                <a href="https://pro-vider.github.io/personal-website/">&#10025;</a>
                <a href="http://avinashpaliwal.com/">&#10025;</a>
                <a href="https://soham-official.github.io/portfolio/">&#10025;</a>
                <a href="https://cdmdc.github.io/">&#10025;</a>
                <a href="http://www.wisdom.weizmann.ac.il/~/assafsho/">&#10025;</a>
                <a href="https://yotamnitzan.github.io/">&#10025;</a>
                <a href="https://www.zijianwang.me/">&#10025;</a>
                <a href="https://people.rit.edu/hv8322/">&#10025;</a>
                <a href="https://ilyac.info/">&#10025;</a>
                <a href="https://tamarott.github.io/">&#10025;</a>
                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/">&#10025;</a>
                <a href="https://stanford.edu/~amywang1/">&#10025;</a>
                <a href="https://g1910.github.io/">&#10025;</a>
                <a href="http://ozzie00.github.io/">&#10025;</a>
                <a href="http://xiomarag.github.io">&#10025;</a>
                <a href="http://plalsanjay.github.io">&#10025;</a>
                <a href="https://vr25.github.io/">&#10025;</a>
                <a href="https://vishal-keshav.github.io">&#10025;</a>
                <a href="https://zhihualiued.github.io">&#10025;</a>
                <a href="https://lczong.github.io/">&#10025;</a>
                <a href="https://timkimd.github.io/">&#10025;</a>
		            <a href="https://hyekangjoo.github.io/">&#10025;</a>
		            <a href="https://nivha.github.io/">&#10025;</a>
		            <a href="https://litun5315.github.io/">&#10025;</a>
		            <a href="https://pursuerchen.github.io/">&#10025;</a>
		            <a href="https://shubhamdongriyal.github.io/Homepage/">&#10025;</a>
		            <a href="https://digantamisra98.github.io/">&#10025;</a>
		            <a href="https://zhunzhong.com/">&#10025;</a>
		            <a href="https://web.mit.edu/bauza/www/">&#10025;</a>
		            <a href="https://shjung13.github.io">&#10025;</a>
		            <a href="https://analogicalnexus.github.io">&#10025;</a>
		            <a href="http://yjzheng.com">&#10025;</a>
		            <a href="http://kiranramnath007.github.io">&#10025;</a>
		      	    <a href="https://fdlandi.github.io/">&#10025;</a>
		      	    <a href="https://sherrycattt.github.io/">&#10025;</a>
		            <a href="http://petersen.ai">&#10025;</a>
		            <a href="https://habibslim.github.io/">&#10025;</a>
		            <a href="https://leoshine.github.io/">&#10025;</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
